{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e75b74a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/antonum/Timescale-Workshops/blob/main/Tutorials/analyze_financial_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d785e",
   "metadata": {},
   "source": [
    "# Analyze financial tick data\n",
    "\n",
    "\n",
    " This notebook is an adaptation of https://docs.timescale.com/tutorials/latest/financial-tick-data/\n",
    "\n",
    " The financial industry is extremely data-heavy and relies on real-time and historical data for decision-making, risk assessment, fraud detection, and market analysis. Timescale simplifies management of these large volumes of data, while also providing you with meaningful analytical insights and optimizing storage costs.\n",
    "  \n",
    "  To analyze financial data, you can chart the open, high, low, close, and volume (OHLCV) information for a financial asset. Using this data, you can create candlestick charts that make it easier to analyze the price changes of financial assets over time. You can use candlestick charts to examine trends in stock, cryptocurrency, or NFT prices.\n",
    "  \n",
    " In this tutorial, you use real raw financial data provided by Twelve Data, create an aggregated candlestick view, query the aggregated data.\n",
    " \n",
    " ## Prerequisites\n",
    " To follow the steps on this page:\n",
    "\n",
    " Create a target Timescale Cloud service with time-series and analytics enabled.\n",
    " https://console.cloud.timescale.com/signup\n",
    " \n",
    " You need your connection details like `\"postgres://tsdbadmin:xxxxxxx.yyyyy.tsdb.cloud.timescale.com:39966/tsdb?sslmode=require\"` \n",
    " \n",
    " Alternatevely you can use self-hosted TimescaleDB. In the Google Colab environment we will install TimescaleDB and use it as a local database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825c345",
   "metadata": {},
   "source": [
    "## About OHLCV data and candlestick charts\n",
    " The financial sector regularly uses candlestick charts to visualize the price change of an asset. \n",
    " Each candlestick represents a time period, such as one minute or one hour, and shows how the asset's price changed during that time.\n",
    "\n",
    " Candlestick charts are generated from the open, high, low, close, and volume data for each financial asset during the time period. \n",
    " This is often abbreviated as OHLCV:\n",
    " - Open: opening price\n",
    " - High: highest price\n",
    " - Low: lowest price\n",
    " - Close: closing price\n",
    " - Volume: volume of transactions\n",
    "\n",
    " ![candlestick data](https://assets.timescale.com/docs/images/tutorials/intraday-stock-analysis/timescale_cloud_candlestick.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52765a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "### Default connection for in-notebook Timescale ###\n",
    "TS_CONNECTION=\"postgres://postgres:password@localhost/postgres\"\n",
    "\n",
    "### Use environment variable ###\n",
    "#TS_CONNECTION = os.getenv(\"TS_CONNECTION\", \"postgres://postgres:password@localhost/postgres\")\n",
    "\n",
    "### Use your own Timescale Cloud instance ###\n",
    "#TS_CONNECTION=\"postgres://tsdbadmin:xxxxxxx.yyyyy.tsdb.cloud.timescale.com:39966/tsdb?sslmode=require\"\n",
    "\n",
    "### Use colab secret ###\n",
    "#from google.colab import userdata\n",
    "#TS_CONNECTION=userdata.get('TS_CONNECTION')\n",
    "\n",
    "### Set environment variable to be used in psql CLI ###\n",
    "os.environ[\"TS_CONNECTION\"]=TS_CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fe3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install Timescale (takes 2-3 minutes)\n",
    "%%bash\n",
    "set -e # Exit immediately if a command exits with a non-zero status.\n",
    "\n",
    "# --- Configuration ---\n",
    "PG_VERSION=\"17\"\n",
    "PGVECTORSCALE_VERSION=\"0.7.0\"\n",
    "PG_PASSWORD=\"password\" # Consider using a more secure password\n",
    "\n",
    "echo \"--- 1. Installing Prerequisites & Adding Repositories ---\"\n",
    "# Install essential packages quietly\n",
    "apt-get -qq -y install gnupg postgresql-common apt-transport-https lsb-release wget > /dev/null 2>&1\n",
    "\n",
    "# Add the official PostgreSQL repository\n",
    "# The 'yes |' answers confirmation prompts automatically. Output redirected.\n",
    "yes | /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh > /dev/null 2>&1\n",
    "\n",
    "# Add the TimescaleDB repository\n",
    "echo \"deb https://packagecloud.io/timescale/timescaledb/ubuntu/ $(lsb_release -c -s) main\" | sudo tee /etc/apt/sources.list.d/timescaledb.list > /dev/null\n",
    "# Add the TimescaleDB GPG key using the recommended method (avoids apt-key add)\n",
    "wget --quiet -O - https://packagecloud.io/timescale/timescaledb/gpgkey | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/timescaledb.gpg\n",
    "\n",
    "echo \"--- 2. Updating Package List & Installing PostgreSQL + Extensions ---\"\n",
    "# Update package list quietly (should suppress apt-key warnings too)\n",
    "apt-get -qq update > /dev/null 2>&1\n",
    "\n",
    "# Install PostgreSQL, TimescaleDB, pgvector, toolkit, and client\n",
    "apt-get -qq -y install \\\n",
    "  \"timescaledb-2-postgresql-${PG_VERSION}\" \\\n",
    "  \"postgresql-client-${PG_VERSION}\" \\\n",
    "  \"postgresql-${PG_VERSION}-pgvector\" \\\n",
    "  \"timescaledb-toolkit-postgresql-${PG_VERSION}\" > /dev/null 2>&1\n",
    "\n",
    "echo \"--- 3. Installing pgvectorscale ---\"\n",
    "# Download and install pgvectorscale\n",
    "wget --quiet \"https://github.com/timescale/pgvectorscale/releases/download/${PGVECTORSCALE_VERSION}/pgvectorscale-${PGVECTORSCALE_VERSION}-pg${PG_VERSION}-amd64.zip\" -O pgvectorscale.zip\n",
    "unzip -q pgvectorscale.zip # Use -q for quiet unzip\n",
    "# Install the .deb package quietly\n",
    "apt-get -qq -y install \"./pgvectorscale-postgresql-${PG_VERSION}_${PGVECTORSCALE_VERSION}-Linux_amd64.deb\" > /dev/null 2>&1\n",
    "\n",
    "# Clean up downloaded files\n",
    "rm pgvectorscale.zip \"./pgvectorscale-postgresql-${PG_VERSION}_${PGVECTORSCALE_VERSION}-Linux_amd64.deb\"\n",
    "\n",
    "echo \"--- 4. Configuring PostgreSQL & TimescaleDB ---\"\n",
    "# Tune PostgreSQL for TimescaleDB\n",
    "timescaledb-tune --quiet --yes  > /dev/null 2>&1\n",
    "\n",
    "# Restart PostgreSQL service to apply changes\n",
    "service postgresql restart\n",
    "sleep 2 # Give the service a moment to restart fully\n",
    "\n",
    "echo \"--- 5. Setting Up Database User and Extensions ---\"\n",
    "# Set the password for the default postgres user\n",
    "sudo -u postgres psql -c \"ALTER USER postgres PASSWORD '${PG_PASSWORD}'\" > /dev/null\n",
    "\n",
    "# Connect as the postgres user and create extensions quietly\n",
    "psql -d \"postgres://postgres:${PG_PASSWORD}@localhost/postgres\" > /dev/null <<EOF\n",
    "CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;\n",
    "CREATE EXTENSION IF NOT EXISTS timescaledb_toolkit CASCADE;\n",
    "CREATE EXTENSION IF NOT EXISTS vector CASCADE;\n",
    "CREATE EXTENSION IF NOT EXISTS vectorscale CASCADE;\n",
    "EOF\n",
    "\n",
    "echo \"--- Installation and Setup Complete ---\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Init psycopg2 connection to Timescale\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import time\n",
    "\n",
    "# establish connection to Timescale\n",
    "conn = psycopg2.connect(TS_CONNECTION)\n",
    "cursor = conn.cursor()\n",
    "conn.autocommit = True\n",
    "\n",
    "\n",
    "# helper function to convert SQL Results to the dataframe\n",
    "def execute_sql(query, cursor=cursor):\n",
    "    try:\n",
    "        time_start = time.time()\n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "        time_end = time.time()\n",
    "        print(f\"Query executed in {round(time_end - time_start,3)} seconds\")\n",
    "        # Check if query returns data (SELECT)\n",
    "        if cursor.description:  # If description is not None, query returned data\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            return df\n",
    "        else:\n",
    "            # Query was likely INSERT, CREATE TABLE, UPDATE, DELETE, etc.\n",
    "            return f\"Rows affected: {cursor.rowcount}\"  # Return the number of rows affected\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing SQL query: {e}\")\n",
    "        conn.rollback()  # Rollback changes in case of error\n",
    "        return None  # Or raise the exception if you prefer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1961e63",
   "metadata": {},
   "source": [
    "## Setup\n",
    " Drop tables and associated objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE IF EXISTS crypto_ticks CASCADE;\n",
    "DROP TABLE IF EXISTS crypto_assets CASCADE;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b16fb",
   "metadata": {},
   "source": [
    "Create a standard PostgreSQL table to store the real-time cryptocurrency data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE crypto_ticks (\n",
    "    \"time\" TIMESTAMPTZ,\n",
    "    symbol TEXT,\n",
    "    price DOUBLE PRECISION,\n",
    "    day_volume NUMERIC\n",
    ");\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4630aa",
   "metadata": {},
   "source": [
    "Convert the standard table into a hypertable partitioned on the time column using the create_hypertable() function provided by Timescale. \n",
    " You must provide the name of the table and the column in that table that holds the timestamp data to use for partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28149c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT create_hypertable('crypto_ticks', by_range('time'));\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e399f",
   "metadata": {},
   "source": [
    "Create a standard PostgreSQL table for relational data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE crypto_assets (\n",
    "    symbol TEXT UNIQUE,\n",
    "    \"name\" TEXT\n",
    ");\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a6533",
   "metadata": {},
   "source": [
    "## Load financial data\n",
    " This tutorial uses real-time cryptocurrency data, also known as tick data, from Twelve Data. \n",
    " To ingest data into the tables that you created, you need to download the dataset, \n",
    " then upload the data to your Timescale Cloud service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6715058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql -d $TS_CONNECTION -c '\\! wget https://assets.timescale.com/docs/downloads/candlestick/crypto_sample.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql -d $TS_CONNECTION -c '\\! unzip -o crypto_sample.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa958b",
   "metadata": {},
   "source": [
    "Load the CSV files into the tables (takes 30-40 seconds)\n",
    " At the psql prompt, use the COPY command to transfer data into your Timescale instance. \n",
    " If the .csv files aren't in your current directory, specify the file paths in these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql -d $TS_CONNECTION -c '\\COPY crypto_ticks FROM '\\''tutorial_sample_tick.csv'\\'' CSV HEADER;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbaec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql -d $TS_CONNECTION -c '\\COPY crypto_assets FROM '\\''tutorial_sample_assets.csv'\\'' CSV HEADER;'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562183c",
   "metadata": {},
   "source": [
    "## Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617488dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM crypto_ticks LIMIT 10;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774de8a7",
   "metadata": {},
   "source": [
    "Preview the reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM crypto_assets LIMIT 10;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a0d8a",
   "metadata": {},
   "source": [
    "## Create indexes\n",
    " Indexes are used to speed up the retrieval of data from a database table.\n",
    " In this case, you create an index on the symbol column of the crypto_assets and crypto_ticks tables.\n",
    " Hypertables automatically create indexes on the time column, so you don't need to create an index on that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE INDEX ON crypto_assets (symbol);\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6caf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE INDEX ON crypto_ticks (symbol);\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da11cc",
   "metadata": {},
   "source": [
    "## Examine hypertable details (psql command)\n",
    " Note that hypertable includes multiple partitions (chunks) of data.\n",
    "\n",
    " You can also see two indices created on the time column and the symbol column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql -d $TS_CONNECTION -c '\\d+ crypto_ticks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20871740",
   "metadata": {},
   "source": [
    "## Examine Hypertable partitions\n",
    " Timescale provides SQL API (functions, views, procedures) to manage hypertables and chunks.\n",
    " The timescaledb_information.chunks view provides information about the chunks of a hypertable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    chunk_name, \n",
    "    range_start, \n",
    "    range_end, \n",
    "    is_compressed \n",
    "FROM \n",
    "    timescaledb_information.chunks\n",
    "WHERE \n",
    "    hypertable_name = 'crypto_ticks';\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d9d67",
   "metadata": {},
   "source": [
    "## JOIN Hypertable and regular table\n",
    " While organized differently internally, hypertables are fully-featured PostgreSQL tables.\n",
    " You can use standard SQL to query the data in a hypertable, including joining it with other tables.\n",
    " In this example, you join the crypto_ticks hypertable with the crypto_assets table to get the name of the asset.\n",
    "\n",
    " Optionally add EXPLAIN ANALYZE to see the query plan.\n",
    " You would see that the query goes through internal chunks of the hypertable like `_hyper_60_285_chunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "-- EXPLAIN ANALYZE \n",
    "SELECT t.time, t.symbol, t.price, t.day_volume, a.name\n",
    "FROM crypto_ticks t\n",
    "JOIN crypto_assets a\n",
    "ON t.symbol = a.symbol\n",
    "ORDER BY t.time DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706258b",
   "metadata": {},
   "source": [
    "Enable timing tracking for the PSQL session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql -d $TS_CONNECTION -c '\\timing on'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2bcb36",
   "metadata": {},
   "source": [
    "## Calculate one-day candlestick data on non-compresssed Hypertable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    time_bucket('1 day', time) AS bucket,\n",
    "    symbol,\n",
    "    FIRST(price, time) AS \"open\",\n",
    "    MAX(price) AS high,\n",
    "    MIN(price) AS low,\n",
    "    LAST(price, time) AS \"close\",\n",
    "    LAST(day_volume, time) AS day_volume\n",
    "FROM crypto_ticks\n",
    "WHERE symbol = 'BTC/USD' AND time >= NOW() - INTERVAL '14 days'\n",
    "GROUP BY bucket, symbol\n",
    "ORDER BY bucket;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c55b7",
   "metadata": {},
   "source": [
    "Remember the time it took to run the query. Later we will compare the performance of the same query \n",
    " on compressed data and preaggregated data in Continuos aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1def853",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install mplfinance\n",
    "import mplfinance as mpf\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    time_bucket('1 day', time) AS bucket,\n",
    "    symbol,\n",
    "    FIRST(price, time) AS \"open\",\n",
    "    MAX(price) AS high,\n",
    "    MIN(price) AS low,\n",
    "    LAST(price, time) AS \"close\",\n",
    "    LAST(day_volume, time) AS day_volume\n",
    "FROM crypto_ticks\n",
    "WHERE symbol = 'BTC/USD' AND time >= NOW() - INTERVAL '14 days'\n",
    "GROUP BY bucket, symbol\n",
    "ORDER BY bucket;\n",
    "\"\"\"\n",
    "df=execute_sql(query)\n",
    "df_plot = df.sort_values('bucket')\n",
    "df_plot = df_plot.set_index('bucket')\n",
    "df_plot = df_plot.rename(columns={\n",
    "    'open': 'Open',\n",
    "    'high': 'High',\n",
    "    'low': 'Low',\n",
    "    'close': 'Close',\n",
    "    'day_volume': 'Volume'\n",
    "})\n",
    "df_plot[\"Volume\"] = pd.to_numeric(df_plot[\"Volume\"], errors='coerce')\n",
    "\n",
    "# Create the candlestick chart\n",
    "mpf.plot(df_plot, type='candle', style='yahoo', volume=False, title='BTC/USD 1-Day Candlestick Chart (Last 14 Days)')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39b751",
   "metadata": {},
   "source": [
    "## Enable Columnarstore (Compression)\n",
    " To enable columnarstore, you need to set the timescaledb.enable_columnstore parameter to true.\n",
    " This parameter is set at the table level, so you need to run the ALTER TABLE command on the crypto_ticks hypertable.\n",
    " The timescaledb.compress_orderby parameter specifies the order in which the data is compressed.\n",
    " The timescaledb.segmentby parameter specifies the column by which the data is segmented.\n",
    " The segmentby column is used to group the data into segments, which are then compressed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5afc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "ALTER TABLE crypto_ticks \n",
    "SET (\n",
    "    timescaledb.enable_columnstore = true, \n",
    "    timescaledb.segmentby = 'symbol',\n",
    "    timescaledb.compress_orderby='time DESC'\n",
    ");\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c6ddb",
   "metadata": {},
   "source": [
    "Enambling a columnarstore for the table by itself does not compress the data.\n",
    " You can either manually compress hypertable chunks or create a policy to automatically compress chunks.\n",
    " The compress_chunk() function compresses the chunk of data in the hypertable.\n",
    " \n",
    " ### Manually compress all the chunks of the hypertable\n",
    " TODO: switch to convert_to_columnarstore()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT compress_chunk(c, true) FROM show_chunks('crypto_ticks') c;\n",
    " -- SELECT decompress_chunk(c, true) FROM show_chunks('crypto_ticks') c;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e06b6",
   "metadata": {},
   "source": [
    "### Automatically compress Hypertable with a policy\n",
    " Create a job that automatically converts chunks in a hypertable to the columnstore older than 1 day\n",
    " This is a preferred way to compress data in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93971a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CALL add_columnstore_policy('crypto_ticks', after => INTERVAL '1d');\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba5ae3",
   "metadata": {},
   "source": [
    "## Storage saved by the compression\n",
    " The hypertable_compression_stats() function returns the size of the compressed and uncompressed data in the hypertable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "  pg_size_pretty(before_compression_total_bytes) as before,\n",
    "  pg_size_pretty(after_compression_total_bytes) as after\n",
    "FROM hypertable_compression_stats('crypto_ticks');\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45993583",
   "metadata": {},
   "source": [
    "In our case the compression ratio is ~10x\n",
    " \n",
    " In practice that means that to store 1TB of data in a hypertable, you need just 100GB of storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35479850",
   "metadata": {},
   "source": [
    "## Calculate one-day candlestick data on compresssed Hypertable\n",
    " This is the same query as above, but now it runs on compressed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b570b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    time_bucket('1 day', time) AS bucket,\n",
    "    symbol,\n",
    "    FIRST(price, time) AS \"open\",\n",
    "    MAX(price) AS high,\n",
    "    MIN(price) AS low,\n",
    "    LAST(price, time) AS \"close\",\n",
    "    LAST(day_volume, time) AS day_volume\n",
    "FROM crypto_ticks\n",
    "WHERE symbol = 'BTC/USD' AND time >= NOW() - INTERVAL '14 days'\n",
    "GROUP BY bucket, symbol\n",
    "ORDER BY bucket;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ce1fb",
   "metadata": {},
   "source": [
    "The query runs on columnar/compressed data and it is faster then the same query on uncompressed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964efd42",
   "metadata": {},
   "source": [
    "## Create a continuous aggregate \n",
    " Continuous aggregates are a TimescaleDB feature that allows you to pre-aggregate data in a hypertable and store the results in a materialized view.\n",
    " This allows you to query the pre-aggregated data instead of the raw data, which can significantly improve query performance. \n",
    " Continuous aggregates are automatically updated as new data is ingested into the hypertable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE MATERIALIZED VIEW one_day_candle\n",
    "WITH (timescaledb.continuous, timescaledb.materialized_only = false) AS\n",
    "    SELECT\n",
    "        time_bucket('1 day', time) AS bucket,\n",
    "        symbol,\n",
    "        FIRST(price, time) AS \"open\",\n",
    "        MAX(price) AS high,\n",
    "        MIN(price) AS low,\n",
    "        LAST(price, time) AS \"close\",\n",
    "        LAST(day_volume, time) AS day_volume\n",
    "    FROM crypto_ticks\n",
    "    GROUP BY bucket, symbol;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377dfe75",
   "metadata": {},
   "source": [
    "### Create continuous aggregate policy\n",
    " The add_continuous_aggregate_policy() function creates a policy that automatically refreshes the continuous aggregate view.\n",
    "\n",
    " The start_offset and end_offset parameters specify the time range for the job, updating the aggregate view.\n",
    "\n",
    " The schedule_interval parameter specifies how often the continuous aggregate view is refreshed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef071b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT add_continuous_aggregate_policy('one_day_candle',\n",
    "    start_offset => INTERVAL '3 days',\n",
    "    end_offset => INTERVAL '1 day',\n",
    "    schedule_interval => INTERVAL '1 day');\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92ed8e",
   "metadata": {},
   "source": [
    "## Query continuous aggregate\n",
    " This query delivers the same results as the previous query, \n",
    " but it runs on the continuous aggregate view instead of the raw data.\n",
    " it is significantly faster than the same query on the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM one_day_candle\n",
    "WHERE symbol = 'BTC/USD' AND bucket >= NOW() - INTERVAL '14 days'\n",
    "ORDER BY bucket;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28efe9e6",
   "metadata": {},
   "source": [
    "## Real Time Continuous Aggregates\n",
    " The continuous aggregate view is automatically updated as new data is ingested into the hypertable.\n",
    " Let's insert a new row into the crypto_ticks table and see how the continuous aggregate view is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "INSERT INTO crypto_ticks (time, symbol, price, day_volume)\n",
    "VALUES (NOW()+INTERVAL '1day', 'BTC/USD', 110000, 30750246);\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM one_day_candle\n",
    "WHERE symbol = 'BTC/USD' AND bucket >= NOW() - INTERVAL '14 days'\n",
    "ORDER BY bucket;\n",
    "\"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba46bc8",
   "metadata": {},
   "source": [
    "As you can see, the continuous aggregate view is automatically updated with the new data.\n",
    " This is the stark contrast to standard Postgres Materialized view that needs to be refreshed manually\n",
    " and does not support real-time updates."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
